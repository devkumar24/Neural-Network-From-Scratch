{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code-NN\n",
    " - It is way similar to tensorflow model but not as tensorflow, tensorflow is more advance than \n",
    "    this, but I am improving this a lot that It should compete tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import make_blobs,make_circles,make_classification\n",
    "from one_hot_enocding import one_hot_encoding\n",
    "from activation import softmax,ReLu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array([\n",
    "    [0,0],\n",
    "    [1,0],\n",
    "    [0,1],\n",
    "    [1,1]\n",
    "])\n",
    "y = np.array([0,1,1,0])\n",
    "y = y.reshape(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((4, 2), (4, 1))"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape,y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x396 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.scatter(X[:,0],X[:,1],c = y)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP:\n",
    "    \n",
    "    def __init__(self,hidden_layers,output_layer,input_layer,activation = \"relu\",output_layer_activation = \"softmax\"):\n",
    "        # Dictionary of all layers with its weight and bias\n",
    "        model_layers = {}\n",
    "        # make self varible for activation\n",
    "        self.activation = activation\n",
    "        # Total no of layers include input + hidden + output layers \n",
    "        total_layers = 1 + len(hidden_layers) + 1\n",
    "        # layers list [input_layer,hidden_layer,output_layer]\n",
    "        # layers[0]--->input_layer\n",
    "        #layers[total_layer-1]--->output_layer\n",
    "        layers = []\n",
    "        layers.append(input_layer)\n",
    "        for i in hidden_layers:\n",
    "            layers.append(i)\n",
    "        layers.append(output_layer)\n",
    "        layers = np.array(layers)\n",
    "        for layer_no in range(total_layers-1):\n",
    "            model_layers[layer_no] = [np.random.randn(layers[layer_no],layers[layer_no+1]),np.zeros((1,layers[layer_no+1]))]\n",
    "        # make class variable self.model_layers to store model_layers\n",
    "        self.model_layers = model_layers\n",
    "        \n",
    "    # function to do forward pass\n",
    "    def forward_propagation(self,X):\n",
    "        '''\n",
    "        This function forward_propagation will make model variable that is having \n",
    "        self.model_layers which includes weight and bias for particular layer,\n",
    "        then it will 4 list i.e., activation units,weight matrix,bias matrix and \n",
    "        Z matrix, and it will return last activation unit.\n",
    "        \n",
    "        \n",
    "        Take an input X(array) and do forward pass operation and return last activation\n",
    "        function.\n",
    "        \n",
    "        example ->\n",
    "        Input: X(array)\n",
    "        Operation: hidden_layers = [4,3]\n",
    "                   input_layer = 2\n",
    "                   output_layer = 2\n",
    "                   \n",
    "                   total_layers = 4\n",
    "                   layers = [2,4,3,2]\n",
    "                   \n",
    "                   Z[0] = X*W[0] + b[0]\n",
    "                   A[0] = activation_function(Z[0])\n",
    "                   \n",
    "                   Z[1] = A[1]*W[1] + b[1]\n",
    "                   A[1] = activation_function(Z[1])\n",
    "        \n",
    "                   Z[2] = A[1]*W[2] + b[2]\n",
    "                   A[2] = activation_function(Z[2])\n",
    "                   \n",
    "        Output: A[2]\n",
    "        '''\n",
    "        # make model variable of self.model_layers\n",
    "        model  = self.model_layers\n",
    "        # make 4 lists of weight metrics, bias metrics, activation metrics, Z metrics\n",
    "        W = []\n",
    "        b = []\n",
    "        Z = []\n",
    "        A = []\n",
    "        # append values in Weight metrics and bias metrics\n",
    "        for key in model.keys():\n",
    "            W.append(model[key][0])\n",
    "            b.append(model[key][1])\n",
    "        W = np.array(W)\n",
    "        # append values in activation units metrics and Z mertics \n",
    "        for i in range(W.shape[0]):\n",
    "            if i == 0:\n",
    "                Z.append(np.dot(X,W[0]) + b[0])\n",
    "                A.append(np.tanh(Z[0]))\n",
    "                \n",
    "            else :\n",
    "                if i == W.shape[0] - 1:\n",
    "                    Z.append(np.dot(A[i-1],W[i]) + b[i])\n",
    "                    A.append(softmax(Z[i]))\n",
    "                else:\n",
    "                    Z.append(np.dot(A[i-1],W[i]) + b[i])\n",
    "                    A.append(np.tanh(Z[i]))\n",
    "                    \n",
    "        # make class variables of all four lists\n",
    "        self.activation_units = (A)\n",
    "        self.W = (W)\n",
    "        self.b = (b)\n",
    "        self.Z = (Z)\n",
    "        \n",
    "        return A[-1]\n",
    "    \n",
    "    # function to do backpropagation in Multi Layer Perceptron\n",
    "    def backward_propagation(self,X,y,learning_rate = 0.01):\n",
    "        '''\n",
    "        This function backward_propagation is initally taking self.model_layers,\n",
    "        self.W,self.b,self.activation_units these 4 will help to do backpropagation \n",
    "        in Multi Layer Perceptron.\n",
    "        \n",
    "        As above 4 things will help in computing dZ,dW,db i.e., the derivatives of Z metrics,\n",
    "        Weight metrics and bias metrics and later perform gradient descent algo. and\n",
    "        update weight and biases\n",
    "        \n",
    "        It takes an input as X(input array) and its prediction array(y), and takes an\n",
    "        input hyper-parameter learning_rate to perform gradient descent algo.\n",
    "        \n",
    "        Therefore, \n",
    "            Backpropagation is basiaclly perform gardient descent algo, and is to\n",
    "            compute dZ,dW,db \n",
    "        example ->\n",
    "        Input: X(input array)\n",
    "        Operation: To Compute dZ,dW,db\n",
    "                   we need W,b,Z,A\n",
    "                   \n",
    "                   dZ[2] = A[2] - y\n",
    "                   dW[2] = (A[1].dZ[2])\n",
    "                   db[2] = dZ[2]\n",
    "                   \n",
    "                   dZ[1] = derivative of activation_function * (dZ[2]*W[2])\n",
    "                   dW[1] = (A[0].dZ[1])\n",
    "                   db[1] = dZ[1]\n",
    "                   \n",
    "                   dZ[0] = derivative of activation_function * (dZ[1]*W[1])\n",
    "                   dW[0] = (X.dZ[0])\n",
    "                   db[0] = dZ[0]\n",
    "                   \n",
    "        Output: Perfrom Gradient Descent Algo\n",
    "                W[i] -= learning_rate * dW[i]\n",
    "                b[i] -= learning_rate * db[i]\n",
    "        '''\n",
    "        # make model variable of self.model_layers\n",
    "        model  = self.model_layers\n",
    "        W = self.W\n",
    "        b = self.b\n",
    "        A = self.activation_units\n",
    "        dZ = []\n",
    "        db = []\n",
    "        dW = []\n",
    "        # calculate dZ\n",
    "        for i in reversed(range(W.shape[0])):\n",
    "            if i == W.shape[0] - 1:\n",
    "                dZ.append(A[i] - y)    \n",
    "            else:\n",
    "                if i == 0:\n",
    "                    dZ.append( (1-np.square(A[i])) * np.dot(dZ[W.shape[0]-i-2],W[i+1].T) )\n",
    "                else:\n",
    "                    dZ.append( (1-np.square(A[i])) * np.dot(dZ[W.shape[0]-i-2],W[i+1].T) )\n",
    "        # reverse dZ so that dZ will get be related to dW and db\n",
    "        dZ = dZ[::-1]\n",
    "        # calculate dW and db\n",
    "        for i in range(W.shape[0]):\n",
    "            if i == W.shape[0] - 1:\n",
    "                dW.append(np.dot(A[i-1].T,dZ[i]))\n",
    "                db.append(np.sum(dZ[i],axis = 0))    \n",
    "            else:\n",
    "                if i == 0:\n",
    "                    dW.append(np.dot(X.T,dZ[i]))\n",
    "                    db.append(np.sum(dZ[i],axis = 0))\n",
    "                else:\n",
    "                    dW.append(np.dot(A[i-1].T,dZ[i]))\n",
    "                    db.append(np.sum(dZ[i],axis = 0))\n",
    "            \n",
    "            # perform gradient descent algo.\n",
    "            W[i] -= learning_rate * dW[i]\n",
    "            b[i] -= learning_rate * db[i]\n",
    "        # class variable to store W and b, after updation\n",
    "        self.W = W\n",
    "        self.b = b\n",
    "        \n",
    "    # Predict function to perform prediction\n",
    "    def predict(self,X):\n",
    "        '''\n",
    "        This function will takes an x_query as input and return the output as per\n",
    "        given y(prediction)\n",
    "        \n",
    "        Return:\n",
    "            1: Probabilities \n",
    "            2: index of max probability\n",
    "        '''\n",
    "        y_out = self.forward_propagation(X)\n",
    "        \n",
    "        return y_out,np.argmax(y_out,axis = 1)\n",
    "    \n",
    "    # Loss Function to calculate loss\n",
    "    def loss(self,y_opt,p):\n",
    "        '''\n",
    "        It will calculate mean squared loss, i.e., categorical cross_entropy loss\n",
    "        \n",
    "        retuurn loss\n",
    "        '''\n",
    "        l = np.mean(y_opt*np.log(p))\n",
    "        return -l\n",
    "    \n",
    "    # Training Function\n",
    "    def train(self,X,y,batch_size,epochs,metrics):\n",
    "        '''\n",
    "        This Train function is basically doing training and perform particular metrics like \n",
    "        \"accurcay\",\"r2Score\", depend on what we calculate, \n",
    "        and it will predict loss after every loss,\n",
    "        it contains a list of training loss, that contains all the losses(the loss calculated after\n",
    "        every iteration,\n",
    "        example->\n",
    "         X.shape[0] = 500\n",
    "         BATCH_SIZE = 32\n",
    "         NO_OF_ITERATIONS = int(X.shape[0]/BATCH_SIZE)-1\n",
    "         training_loss.shape = (len(NO_OF_ITERATIONS))\n",
    "         ).\n",
    "         \n",
    "        Prediction score will be calcuated after each epoch, and it is also having a numpy.ndarray\n",
    "        that store all the prediction score, for each iteration, and shape is same as training_loss\n",
    "        $ y should be one hot vector\n",
    "        $ training_loss is numpy.ndarray\n",
    "        $ epochs is hyper parameter\n",
    "        $ metrics is hyper parameter\n",
    "        '''\n",
    "        training_loss = []\n",
    "        y_opt = one_hot_encoding(y)\n",
    "        \n",
    "        for i in range(epochs):\n",
    "            print(\"EPOCH-->{}\".format(i+1))\n",
    "            for j in range(int(X.shape[0]/batch_size) - 1):\n",
    "                Y_ = self.forward_propagation(X[j*batch_size:(j+1)*batch_size])\n",
    "                l = self.loss(y_opt[j*batch_size:(j+1)*batch_size],Y_)\n",
    "                self.backward_propagation(X[j*batch_size:(j+1)*batch_size],y_opt[j*batch_size:(j+1)*batch_size])\n",
    "                training_loss.append(l)\n",
    "            print(\" Training Loss----->  \",l)\n",
    "            \n",
    "        y_pred = []\n",
    "        if metrics == \"accuracy\":\n",
    "            for i in range(X.shape[0]):\n",
    "                output,index = self.predict(X[i])\n",
    "                y_pred.append(index)\n",
    "            acc = np.sum(y_pred == y)/y.shape[0]\n",
    "        return training_loss,acc*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MLP([4,3],2,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH-->1\n",
      " Training Loss----->   0.9767028883834902\n",
      "EPOCH-->2\n",
      " Training Loss----->   0.9575038601256787\n",
      "EPOCH-->3\n",
      " Training Loss----->   0.9384425656599039\n",
      "EPOCH-->4\n",
      " Training Loss----->   0.9195277915555208\n",
      "EPOCH-->5\n",
      " Training Loss----->   0.9007700577376262\n",
      "EPOCH-->6\n",
      " Training Loss----->   0.8821812348810107\n",
      "EPOCH-->7\n",
      " Training Loss----->   0.8637742198774904\n",
      "EPOCH-->8\n",
      " Training Loss----->   0.8455626613460621\n",
      "EPOCH-->9\n",
      " Training Loss----->   0.8275607251915063\n",
      "EPOCH-->10\n",
      " Training Loss----->   0.8097828901513279\n",
      "EPOCH-->11\n",
      " Training Loss----->   0.7922437643711073\n",
      "EPOCH-->12\n",
      " Training Loss----->   0.7749579157739165\n",
      "EPOCH-->13\n",
      " Training Loss----->   0.7579397109691625\n",
      "EPOCH-->14\n",
      " Training Loss----->   0.7412031594356505\n",
      "EPOCH-->15\n",
      " Training Loss----->   0.7247617615553277\n",
      "EPOCH-->16\n",
      " Training Loss----->   0.7086283606669411\n",
      "EPOCH-->17\n",
      " Training Loss----->   0.6928150005888478\n",
      "EPOCH-->18\n",
      " Training Loss----->   0.6773327909903244\n",
      "EPOCH-->19\n",
      " Training Loss----->   0.6621917835563356\n",
      "EPOCH-->20\n",
      " Training Loss----->   0.647400862098231\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "([0.34657359027997264,\n",
       "  0.09729964626439017,\n",
       "  0.9767028883834902,\n",
       "  0.3351384670109749,\n",
       "  0.0977917109317851,\n",
       "  0.9575038601256787,\n",
       "  0.32505283153034215,\n",
       "  0.09813819151335203,\n",
       "  0.9384425656599039,\n",
       "  0.31618758481129466,\n",
       "  0.09834923880891366,\n",
       "  0.9195277915555208,\n",
       "  0.3084197313028519,\n",
       "  0.09843532110653669,\n",
       "  0.9007700577376262,\n",
       "  0.3016344137174669,\n",
       "  0.09840695042943315,\n",
       "  0.8821812348810107,\n",
       "  0.29572595909769894,\n",
       "  0.09827446932879655,\n",
       "  0.8637742198774904,\n",
       "  0.2905982014438825,\n",
       "  0.09804789177660055,\n",
       "  0.8455626613460621,\n",
       "  0.28616431013397253,\n",
       "  0.0977367901560846,\n",
       "  0.8275607251915063,\n",
       "  0.2823463063816273,\n",
       "  0.0973502200927985,\n",
       "  0.8097828901513279,\n",
       "  0.27907440372346026,\n",
       "  0.09689667539004022,\n",
       "  0.7922437643711073,\n",
       "  0.2762862686524322,\n",
       "  0.09638406624699364,\n",
       "  0.7749579157739165,\n",
       "  0.2739262658389631,\n",
       "  0.09581971499377977,\n",
       "  0.7579397109691625,\n",
       "  0.2719447286359152,\n",
       "  0.09521036462366901,\n",
       "  0.7412031594356505,\n",
       "  0.2702972785590141,\n",
       "  0.09456219635795345,\n",
       "  0.7247617615553277,\n",
       "  0.2689442057361198,\n",
       "  0.09388085330786113,\n",
       "  0.7086283606669411,\n",
       "  0.2678499145844615,\n",
       "  0.09317146799189964,\n",
       "  0.6928150005888478,\n",
       "  0.2669824340872999,\n",
       "  0.09243869203250506,\n",
       "  0.6773327909903244,\n",
       "  0.2663129891333699,\n",
       "  0.09168672680633513,\n",
       "  0.6621917835563356,\n",
       "  0.2658156278118987,\n",
       "  0.09091935417413688,\n",
       "  0.647400862098231],\n",
       " 75.0)"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.train(X,y,1,20,\"accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
