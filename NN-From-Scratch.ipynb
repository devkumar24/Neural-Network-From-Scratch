{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code-NN\n",
    " - It is way similar to tensorflow model but not as tensorflow, tensorflow is more advance than \n",
    "    this, but I am improving this a lot that It should compete tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "F:\\Anaconda_Files\\Anaconda\\envs\\tensorflow-env\\lib\\importlib\\_bootstrap.py:219: RuntimeWarning: numpy.ufunc size changed, may indicate binary incompatibility. Expected 192 from C header, got 216 from PyObject\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.utils import to_categorical\n",
    "from sklearn.datasets import make_blobs,make_circles,make_classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array([\n",
    "    [0,0],\n",
    "    [1,0],\n",
    "    [0,1],\n",
    "    [1,1]\n",
    "])\n",
    "y = np.array([0,1,1,0])\n",
    "y = y.reshape(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((4, 2), (4, 1))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape,y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.scatter(X[:,0],X[:,1],c = y)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ReLu(Z):\n",
    "    return np.maximum(0,Z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(y):\n",
    "    \"\"\"\n",
    "    softmax = e^y(i)/sum(e^y(i))\n",
    "    \"\"\"\n",
    "    \n",
    "    ea = np.exp(y)\n",
    "    \n",
    "    total = ea/np.sum(ea,axis = 1,keepdims=True)\n",
    "        \n",
    "    return total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP:\n",
    "    \n",
    "    def __init__(self,hidden_layers,output_layer,input_layer,activation = \"relu\",output_layer_activation = \"softmax\"):\n",
    "        # Dictionary of all layers with its weight and bias\n",
    "        model_layers = {}\n",
    "        # make self varible for activation\n",
    "        self.activation = activation\n",
    "        # Total no of layers include input + hidden + output layers \n",
    "        total_layers = 1 + len(hidden_layers) + 1\n",
    "        # layers list [input_layer,hidden_layer,output_layer]\n",
    "        # layers[0]--->input_layer\n",
    "        #layers[total_layer-1]--->output_layer\n",
    "        layers = []\n",
    "        layers.append(input_layer)\n",
    "        for i in hidden_layers:\n",
    "            layers.append(i)\n",
    "        layers.append(output_layer)\n",
    "        layers = np.array(layers)\n",
    "        for layer_no in range(total_layers-1):\n",
    "            model_layers[layer_no] = [np.random.randn(layers[layer_no],layers[layer_no+1]),np.zeros((1,layers[layer_no+1]))]\n",
    "        # make class variable self.model_layers to store model_layers\n",
    "        self.model_layers = model_layers\n",
    "        \n",
    "    # function to do forward pass\n",
    "    def forward_propagation(self,X):\n",
    "        '''\n",
    "        This function forward_propagation will make model variable that is having \n",
    "        self.model_layers which includes weight and bias for particular layer,\n",
    "        then it will 4 list i.e., activation units,weight matrix,bias matrix and \n",
    "        Z matrix, and it will return last activation unit.\n",
    "        \n",
    "        \n",
    "        Take an input X(array) and do forward pass operation and return last activation\n",
    "        function.\n",
    "        \n",
    "        example ->\n",
    "        Input: X(array)\n",
    "        Operation: hidden_layers = [4,3]\n",
    "                   input_layer = 2\n",
    "                   output_layer = 2\n",
    "                   \n",
    "                   total_layers = 4\n",
    "                   layers = [2,4,3,2]\n",
    "                   \n",
    "                   Z[0] = X*W[0] + b[0]\n",
    "                   A[0] = activation_function(Z[0])\n",
    "                   \n",
    "                   Z[1] = A[1]*W[1] + b[1]\n",
    "                   A[1] = activation_function(Z[1])\n",
    "        \n",
    "                   Z[2] = A[1]*W[2] + b[2]\n",
    "                   A[2] = activation_function(Z[2])\n",
    "                   \n",
    "        Output: A[2]\n",
    "        '''\n",
    "        # make model variable of self.model_layers\n",
    "        model  = self.model_layers\n",
    "        # make 4 lists of weight metrics, bias metrics, activation metrics, Z metrics\n",
    "        W = []\n",
    "        b = []\n",
    "        Z = []\n",
    "        A = []\n",
    "        # append values in Weight metrics and bias metrics\n",
    "        for key in model.keys():\n",
    "            W.append(model[key][0])\n",
    "            b.append(model[key][1])\n",
    "        W = np.array(W)\n",
    "        # append values in activation units metrics and Z mertics \n",
    "        for i in range(W.shape[0]):\n",
    "            if i == 0:\n",
    "                Z.append(np.dot(X,W[0]) + b[0])\n",
    "                A.append(np.tanh(Z[0]))\n",
    "                \n",
    "            else :\n",
    "                if i == W.shape[0] - 1:\n",
    "                    Z.append(np.dot(A[i-1],W[i]) + b[i])\n",
    "                    A.append(softmax(Z[i]))\n",
    "                else:\n",
    "                    Z.append(np.dot(A[i-1],W[i]) + b[i])\n",
    "                    A.append(np.tanh(Z[i]))\n",
    "                    \n",
    "        # make class variables of all four lists\n",
    "        self.activation_units = (A)\n",
    "        self.W = (W)\n",
    "        self.b = (b)\n",
    "        self.Z = (Z)\n",
    "        \n",
    "        return A[-1]\n",
    "    \n",
    "    # function to do backpropagation in Multi Layer Perceptron\n",
    "    def backward_propagation(self,X,y,learning_rate = 0.001):\n",
    "        '''\n",
    "        This function backward_propagation is initally taking self.model_layers,\n",
    "        self.W,self.b,self.activation_units these 4 will help to do backpropagation \n",
    "        in Multi Layer Perceptron.\n",
    "        \n",
    "        As above 4 things will help in computing dZ,dW,db i.e., the derivatives of Z metrics,\n",
    "        Weight metrics and bias metrics and later perform gradient descent algo. and\n",
    "        update weight and biases\n",
    "        \n",
    "        It takes an input as X(input array) and its prediction array(y), and takes an\n",
    "        input hyper-parameter learning_rate to perform gradient descent algo.\n",
    "        \n",
    "        Therefore, \n",
    "            Backpropagation is basiaclly perform gardient descent algo, and is to\n",
    "            compute dZ,dW,db \n",
    "        example ->\n",
    "        Input: X(input array)\n",
    "        Operation: To Compute dZ,dW,db\n",
    "                   we need W,b,Z,A\n",
    "                   \n",
    "                   dZ[2] = A[2] - y\n",
    "                   dW[2] = (A[1].dZ[2])\n",
    "                   db[2] = dZ[2]\n",
    "                   \n",
    "                   dZ[1] = derivative of activation_function * (dZ[2]*W[2])\n",
    "                   dW[1] = (A[0].dZ[1])\n",
    "                   db[1] = dZ[1]\n",
    "                   \n",
    "                   dZ[0] = derivative of activation_function * (dZ[1]*W[1])\n",
    "                   dW[0] = (X.dZ[0])\n",
    "                   db[0] = dZ[0]\n",
    "                   \n",
    "        Output: Perfrom Gradient Descent Algo\n",
    "                W[i] -= learning_rate * dW[i]\n",
    "                b[i] -= learning_rate * db[i]\n",
    "        '''\n",
    "        # make model variable of self.model_layers\n",
    "        model  = self.model_layers\n",
    "        W = self.W\n",
    "        b = self.b\n",
    "        A = self.activation_units\n",
    "        dZ = []\n",
    "        db = []\n",
    "        dW = []\n",
    "        # calculate dZ\n",
    "        for i in reversed(range(W.shape[0])):\n",
    "            if i == W.shape[0] - 1:\n",
    "                dZ.append(A[i] - y)    \n",
    "            else:\n",
    "                if i == 0:\n",
    "                    dZ.append( (1-np.square(A[i])) * np.dot(dZ[W.shape[0]-i-2],W[i+1].T) )\n",
    "                else:\n",
    "                    dZ.append( (1-np.square(A[i])) * np.dot(dZ[W.shape[0]-i-2],W[i+1].T) )\n",
    "        # reverse dZ so that dZ will get be related to dW and db\n",
    "        dZ = dZ[::-1]\n",
    "        # calculate dW and db\n",
    "        for i in range(W.shape[0]):\n",
    "            if i == W.shape[0] - 1:\n",
    "                dW.append(np.dot(A[i-1].T,dZ[i]))\n",
    "                db.append(np.sum(dZ[i],axis = 0))    \n",
    "            else:\n",
    "                if i == 0:\n",
    "                    dW.append(np.dot(X.T,dZ[i]))\n",
    "                    db.append(np.sum(dZ[i],axis = 0))\n",
    "                else:\n",
    "                    dW.append(np.dot(A[i-1].T,dZ[i]))\n",
    "                    db.append(np.sum(dZ[i],axis = 0))\n",
    "            \n",
    "            # perform gradient descent algo.\n",
    "            W[i] -= learning_rate * dW[i]\n",
    "            b[i] -= learning_rate * db[i]\n",
    "        # class variable to store W and b, after updation\n",
    "        self.W = W\n",
    "        self.b = b\n",
    "        \n",
    "    # Predict function to perform prediction\n",
    "    def predict(self,X):\n",
    "        '''\n",
    "        This function will takes an x_query as input and return the output as per\n",
    "        given y(prediction)\n",
    "        \n",
    "        Return:\n",
    "            1: Probabilities \n",
    "            2: index of max probability\n",
    "        '''\n",
    "        y_out = self.forward_propagation(X)\n",
    "        \n",
    "        return y_out,np.argmax(y_out,axis = 1)\n",
    "    \n",
    "    # Loss Function to calculate loss\n",
    "    def loss(self,y_opt,p):\n",
    "        '''\n",
    "        It will calculate mean squared loss, i.e., categorical cross_entropy loss\n",
    "        \n",
    "        retuurn loss\n",
    "        '''\n",
    "        l = np.mean(y_opt*np.log(p))\n",
    "        return -l\n",
    "    \n",
    "    # Training Function\n",
    "    def train(self,X,y,batch_size,epochs,metrics):\n",
    "        '''\n",
    "        This Train function is basically doing training and perform particular metrics like \n",
    "        \"accurcay\",\"r2Score\", depend on what we calculate, \n",
    "        and it will predict loss after every loss,\n",
    "        it contains a list of training loss, that contains all the losses(the loss calculated after\n",
    "        every iteration,\n",
    "        example->\n",
    "         X.shape[0] = 500\n",
    "         BATCH_SIZE = 32\n",
    "         NO_OF_ITERATIONS = int(X.shape[0]/BATCH_SIZE)-1\n",
    "         training_loss.shape = (len(NO_OF_ITERATIONS))\n",
    "         ).\n",
    "         \n",
    "        Prediction score will be calcuated after each epoch, and it is also having a numpy.ndarray\n",
    "        that store all the prediction score, for each iteration, and shape is same as training_loss\n",
    "        $ y should be one hot vector\n",
    "        $ training_loss is numpy.ndarray\n",
    "        $ epochs is hyper parameter\n",
    "        $ metrics is hyper parameter\n",
    "        '''\n",
    "        training_loss = []\n",
    "        train_loss_dic = {}\n",
    "        metrics_ = []\n",
    "        metrics_dic = {}\n",
    "        y_opt = to_categorical(y)\n",
    "        \n",
    "        for i in range(epochs):\n",
    "            print(\"EPOCH-->{}\".format(i+1))\n",
    "            for j in range(int(X.shape[0]/batch_size) - 1):\n",
    "                Y_ = self.forward_propagation(X[j*batch_size:(j+1)*batch_size])\n",
    "                l = self.loss(y_opt[j*batch_size:(j+1)*batch_size],Y_)\n",
    "                if metrics == \"accuracy\" or metrics == \"Accuracy\" or metrics == \"ACCURACY\":\n",
    "                    met = (sum(Y_ == y)/y.shape[0])*100\n",
    "                else:\n",
    "                    met = None\n",
    "                self.backward_propagation(X[j*batch_size:(j+1)*batch_size],y_opt[j*batch_size:(j+1)*batch_size])\n",
    "                training_loss.append(l)\n",
    "                metrics_.append(met)\n",
    "                train_loss_dic[i] = l\n",
    "                metrics_dic[i] = l\n",
    "                if train_loss_dic[i] == None:\n",
    "                    train_loss_dic[i] = l\n",
    "                else:\n",
    "                    train_loss_dic[i] = l\n",
    "            print(\" Training Loss----->  \",l)\n",
    "            \n",
    "        \n",
    "        return train_loss_dic,metrics_dic\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MLP(hidden_layers=[4,3],output_layer=2,input_layer=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH-->1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "F:\\Anaconda_Files\\Anaconda\\envs\\tensorflow-env\\lib\\site-packages\\ipykernel_launcher.py:225: DeprecationWarning: elementwise comparison failed; this will raise an error in the future.\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'bool' object is not iterable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-47-2f7597832c9b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"accuracy\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-40-eae52ec0c85f>\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(self, X, y, batch_size, epochs, metrics)\u001b[0m\n\u001b[0;32m    223\u001b[0m                 \u001b[0ml\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_opt\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mj\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mj\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mY_\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    224\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mmetrics\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"accuracy\"\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mmetrics\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"Accuracy\"\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mmetrics\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"ACCURACY\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 225\u001b[1;33m                     \u001b[0mmet\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mY_\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m/\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m*\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    226\u001b[0m                 \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    227\u001b[0m                     \u001b[0mmet\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: 'bool' object is not iterable"
     ]
    }
   ],
   "source": [
    "model.train(X,y,2,10,metrics=\"accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([[-0.00121461,  0.00100665,  0.00622787, -0.00574468]]), array([[-0.0093351 ,  0.01538209,  0.01652933]]), array([[0.49851027, 0.50148973]])]\n",
      "[[0.49851027 0.50148973]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([1], dtype=int64)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(X[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([[-0.01342753,  0.0267609 ,  0.0006956 , -0.00989456]]), array([[-0.03282873,  0.04428288,  0.09311656]]), array([[0.50770326, 0.49229674]])]\n",
      "[[0.50770326 0.49229674]]\n",
      "[array([[-0.09595524, -0.33180878, -0.09967312,  0.50136105]]), array([[ 0.02665382,  0.17334572, -0.83627723]]), array([[0.35964772, 0.64035228]])]\n",
      "[[0.35964772 0.64035228]]\n",
      "[array([[-0.98534361,  0.57675421, -0.02132224,  0.01742284]]), array([[-0.96445039,  0.9722111 ,  0.97640928]]), array([[0.70663376, 0.29336624]])]\n",
      "[[0.70663376 0.29336624]]\n",
      "[array([[-0.98756706,  0.27841087, -0.12142416,  0.52153213]]), array([[-0.94667586,  0.97638624,  0.74967385]]), array([[0.67291706, 0.32708294]])]\n",
      "[[0.67291706 0.32708294]]\n",
      "[array([0], dtype=int64), array([1], dtype=int64), array([0], dtype=int64), array([0], dtype=int64)]\n"
     ]
    }
   ],
   "source": [
    "y_out = []\n",
    "for i in range(X.shape[0]):\n",
    "    y_out.append(model.predict(X[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([0], dtype=int64), array([1], dtype=int64), array([0], dtype=int64), array([0], dtype=int64)]\n"
     ]
    }
   ],
   "source": [
    "print(y_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([75.])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(y_out == y)/4*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0]])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Dense\n",
    "from keras.models import Sequential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
